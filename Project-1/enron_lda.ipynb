{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# for plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "# prep NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10348, 2)\n",
      "                              file  \\\n",
      "186822  jones-t/all_documents/634.   \n",
      "308790  mann-k/all_documents/5690.   \n",
      "82383         dasovich-j/sent/423.   \n",
      "227299          kaminski-v/var/63.   \n",
      "301824     mann-k/_sent_mail/3208.   \n",
      "\n",
      "                                                  message  \n",
      "186822  Message-ID: <17820178.1075846925335.JavaMail.e...  \n",
      "308790  Message-ID: <29110382.1075845717882.JavaMail.e...  \n",
      "82383   Message-ID: <6812040.1075843194135.JavaMail.ev...  \n",
      "227299  Message-ID: <21547648.1075856642126.JavaMail.e...  \n",
      "301824  Message-ID: <12684200.1075846107179.JavaMail.e...  \n"
     ]
    }
   ],
   "source": [
    "# Read the emails\n",
    "emails = pd.read_csv(\"emails.csv\")\n",
    "email_subset = emails.sample(frac=0.02,random_state=1)\n",
    "print(email_subset.shape)\n",
    "print(email_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing data out from the raw messages\n",
    "\n",
    "def parse_raw_message(raw_message):\n",
    "    lines = raw_message.split('\\n')\n",
    "    email = {}\n",
    "    message = ''\n",
    "    keys_to_extract = ['message-id', 'from', 'to']\n",
    "    for line in lines:\n",
    "        if ':' not in line:\n",
    "            message += line.strip()\n",
    "            email['body'] = message\n",
    "        else:\n",
    "            pairs = line.split(':')\n",
    "            key = pairs[0].lower()\n",
    "            val = pairs[1].strip()\n",
    "            if key in keys_to_extract:\n",
    "                email[key] = val\n",
    "    return email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_list(emails, key):\n",
    "    results = []\n",
    "    for email in emails:\n",
    "        if key not in email:\n",
    "            results.append('')\n",
    "        else:\n",
    "            results.append(email[key])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_into_emails(messages):\n",
    "    emails = [parse_raw_message(message) for message in messages]\n",
    "    return {\n",
    "        'body': map_to_list(emails, 'body'),\n",
    "        'to': map_to_list(emails, 'to'),\n",
    "        'from_': map_to_list(emails, 'from'),\n",
    "        'msg_id': map_to_list(emails, 'message-id')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body  \\\n",
      "0  It would be nice if you could be at my dinner,...   \n",
      "1  Absolutely.Good point!  Can Peter start to dra...   \n",
      "2  My apologies.  My schedule melted down after w...   \n",
      "3  Vince,UK VAR breached the limit last week.UK t...   \n",
      "4  Any problems/comments?AM ---------------------...   \n",
      "\n",
      "                                                  to  \\\n",
      "0                           alicia.goodrow@enron.com   \n",
      "1                          Kay Mann/Corp/Enron@ENRON   \n",
      "2                        christine.piesco@oracle.com   \n",
      "3  Richard Lewis/LON/ECT@ECT, James New/LON/ECT@E...   \n",
      "4  Don Hammond/PDX/ECT@ECT, Jody Blackburn/PDX/EC...   \n",
      "\n",
      "                               from_  \\\n",
      "0               tana.jones@enron.com   \n",
      "1  Sheila Tweed@ECT on 05/15/2001 06   \n",
      "2            jeff.dasovich@enron.com   \n",
      "3        tanya.tamarchenko@enron.com   \n",
      "4                 kay.mann@enron.com   \n",
      "\n",
      "                                          msg_id  \n",
      "0  <17820178.1075846925335.JavaMail.evans@thyme>  \n",
      "1  <29110382.1075845717882.JavaMail.evans@thyme>  \n",
      "2   <6812040.1075843194135.JavaMail.evans@thyme>  \n",
      "3  <21547648.1075856642126.JavaMail.evans@thyme>  \n",
      "4  <12684200.1075846107179.JavaMail.evans@thyme>  \n"
     ]
    }
   ],
   "source": [
    "email_df = pd.DataFrame(parse_into_emails(email_subset.message))\n",
    "print(email_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My apologies.  My schedule melted down after we talked on Monday.  Here'swhere folks came out.  There's some concern about size.  We're supposed to beno larger than 3, but we lobbied Aceves and he apparently Ok'd our\"oversized\" group.  The other folks in the group--who talked to himoriginally--are pretty sure that five will violate the rules.  Folks wonderedif there were other groups that are smaller than ours that you could hook upwith.  Sorry about that---it's a wrinkle that I didn't think about when wespoke.  If it gets real ugly trying to find a smaller group, let me know.Fortunately there's not another team case due for two weeks.Best,Jeff\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "print(email_df.iloc[2]['body'])\n",
    "# Convert email body to list\n",
    "data = email_df.body.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize - break down each sentence into a list of words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vince', 'uk', 'var', 'breached', 'the', 'limit', 'last', 'week', 'uk', 'traders', 'asked', 'us', 'to', 'review', 'the', 'correlations', 'across', 'uk', 'gas', 'and', 'power', 'aswell', 'as', 'the', 'correlations', 'across', 'efa', 'slots', 'we', 'did', 'part', 'of', 'the', 'work', 'last', 'week', 'now', 'we', 'll', 'update', 'the', 'correlations', 'based', 'on', 'historical', 'prices', 'tanya', 'richard', 'lewisleppard', 'lon', 'ect', 'ect', 'rudy', 'dautel', 'hou', 'ect', 'ect', 'kirstee', 'hewitt', 'lon', 'ect', 'ect', 'naveen', 'andrews', 'corp', 'enron', 'enron', 'david', 'port', 'market', 'risk', 'corp', 'enron', 'enron', 'tedmurphy', 'hou', 'ect', 'ect', 'simon', 'hastings', 'lon', 'ect', 'ect', 'paul', 'arcy', 'lon', 'ect', 'ect', 'amirghodsian', 'lon', 'ect', 'ectthanks', 'tanya', 'these', 'are', 'interesting', 'results', 'am', 'on', 'vacation', 'next', 'week', 'sohere', 'are', 'my', 'current', 'thoughts', 'am', 'contactable', 'on', 'my', 'mobile', 'if', 'necessary', 'gas', 'to', 'power', 'correlationsi', 'see', 'your', 'point', 'about', 'gas', 'to', 'power', 'correlation', 'only', 'affecting', 'var', 'for', 'theconservative', 'long', 'term', 'correlation', 'combined', 'var', 'is', 'mm', 'less', 'thanpreviously', 'expected', 'so', 'how', 'does', 'this', 'affect', 'the', 'limit', 'breach', 'we', 'are', 'still', 'over', 'our', 'uk', 'power', 'limit', 'but', 'the', 'limit', 'was', 'set', 'when', 'wewere', 'assuming', 'no', 'gas', 'power', 'correlation', 'and', 'therefore', 'higher', 'portfolio', 'var', 'suggested', 'way', 'forward', 'given', 'the', 'importance', 'of', 'the', 'spread', 'options', 'to', 'the', 'ukgas', 'and', 'power', 'books', 'can', 'we', 'allocate', 'to', 'the', 'gas', 'and', 'power', 'books', 'share', 'of', 'the', 'reduction', 'inportfolio', 'var', 'ie', 'reduction', 'portfolio', 'var', 'sum', 'power', 'var', 'gas', 'var', 'also', 'if', 'understand', 'your', 'mail', 'correctly', 'matrix', 'implies', 'gas', 'is', 'consistent', 'with', 'our', 'correlation', 'curves', 'and', 'this', 'reduces', 'totalvar', 'by', 'mm', 'efa', 'slot', 'correlationsthe', 'issue', 'of', 'whether', 'our', 'existing', 'efa', 'to', 'efa', 'correlation', 'matrix', 'is', 'correct', 'isa', 'separate', 'issue', 'don', 'understand', 'where', 'the', 'matrix', 'efa', 'to', 'efacorrelations', 'come', 'from', 'but', 'am', 'happy', 'for', 'you', 'to', 'run', 'some', 'from', 'the', 'forward', 'curves', 'use', 'the', 'first', 'years', 'wouldsuggest', 'our', 'original', 'matrix', 'was', 'based', 'on', 'historicals', 'but', 'the', 'analysis', 'isworth', 'doing', 'again', 'your', 'matrix', 'results', 'certainly', 'indicate', 'how', 'importantthese', 'correlations', 'are', 'closing', 'thoughtsfriday', 'trading', 'left', 'us', 'longer', 'so', 'would', 'not', 'expect', 'limit', 'breach', 'onmonday', 'we', 'are', 'still', 'reviewing', 'the', 'shape', 'of', 'the', 'long', 'term', 'curve', 'and', 'dlike', 'to', 'wait', 'until', 'both', 'simon', 'hastings', 'and', 'are', 'back', 'in', 'the', 'office', 'mondayweek', 'before', 'finalising', 'this', 'tamarchenkonew', 'lon', 'ect', 'ect', 'steven', 'leppard', 'lon', 'ect', 'ect', 'rudy', 'dautel', 'hou', 'ect', 'ect', 'kirsteehewitt', 'lon', 'ect', 'ect', 'naveen', 'andrews', 'corp', 'enron', 'enron', 'david', 'port', 'marketrisk', 'corp', 'enron', 'enron', 'ted', 'murphy', 'hou', 'ect', 'ecteverybody', 'oliver', 'sent', 'us', 'the', 'var', 'number', 'for', 'different', 'correlations', 'for', 'uk', 'powerportfolio', 'separately', 'from', 'uk', 'gas', 'portfolio', 'first', 'if', 'var', 'is', 'calculated', 'accurately', 'the', 'correlation', 'between', 'power', 'and', 'gascurves', 'should', 'not', 'affect', 'var', 'number', 'for', 'power', 'and', 'var', 'number', 'for', 'gas', 'onlythe', 'aggregate', 'number', 'will', 'be', 'affected', 'the', 'changes', 'you', 'see', 'are', 'due', 'to', 'thefact', 'that', 'we', 'use', 'monte', 'carlo', 'simulation', 'method', 'which', 'accuracy', 'depends', 'on', 'the', 'number', 'of', 'simulations', 'even', 'if', 'we', 'don', 'changethe', 'correlations', 'but', 'use', 'different', 'realizations', 'of', 'random', 'numbers', 'we', 'get', 'slightly', 'different', 'result', 'from', 'the', 'model', 'we', 'should', 'look', 'at', 'the', 'aggregate', 'number', 'calculated', 'weighted', 'correlations', 'based', 'on', 'curves', 'got', 'from', 'paul', 'as', 'theweights', 'along', 'the', 'term', 'structure', 'used', 'the', 'product', 'of', 'price', 'position', 'andvolatility', 'for', 'each', 'time', 'bucket', 'for', 'gas', 'and', 'each', 'of', 'efa', 'slots', 'the', 'these', 'numbers', 'into', 'the', 'original', 'correlation', 'matrix', 'definite', 'correlation', 'matrix', 'which', 'brakes', 'var', 'engine', 'correlation', 'matrix', 'for', 'any', 'set', 'of', 'random', 'variables', 'is', 'non', 'negative', 'bydefinition', 'and', 'remains', 'non', 'negatively', 'definite', 'if', 'calculated', 'properly', 'basedon', 'any', 'historical', 'data', 'here', 'according', 'to', 'our', 'phone', 'discussion', 'we', 'started', 'experimenting', 'assuming', 'the', 'same', 'correlation', 'for', 'each', 'efa', 'slot', 'and', 'et', 'elecversus', 'gas', 'am', 'sending', 'you', 'the', 'spreadsheet', 'which', 'summaries', 'the', 'results', 'inaddition', 'to', 'the', 'aggregate', 'var', 'numbers', 'for', 'the', 'runs', 'oliver', 'did', 'you', 'can', 'seethe', 'var', 'numbers', 'based', 'on', 'correlation', 'matrix', 'and', 'matrix', 'in', 'matrix', 'thecorrelations', 'across', 'efa', 'slots', 'are', 'identical', 'to', 'these', 'in', 'original', 'matrix', 'obtained', 'this', 'matrix', 'by', 'trial', 'and', 'error', 'matrix', 'is', 'produces', 'by', 'naveenusing', 'finger', 'algorithm', 'it', 'differs', 'from', 'original', 'matrix', 'across', 'efa', 'slots', 'aswellas', 'in', 'power', 'versus', 'gas', 'correlations', 'and', 'gives', 'higher', 'var', 'than', 'matrix', 'does', 'calculate', 'historical', 'correlations', 'from', 'them', 'tanya', 'oliver', 'gaylardleppard', 'lon', 'ect', 'ect', 'rudy', 'dautel', 'hou', 'ect', 'ect', 'kirstee', 'hewitt', 'lon', 'ect', 'ect', 'naveen', 'andrews', 'corp', 'enron', 'enron', 'tanya', 'tamarchenko', 'hou', 'ect', 'ect', 'davidport', 'market', 'risk', 'corp', 'enron', 'var', 'uk', 'power', 'book', 'var', 'uk', 'gas', 'book', 'mm', 'mm', 'mm', 'mm', 'mm', 'mm', 'mm', 'mm', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'peaks', 'and', 'off', 'peaks', 'were', 'treated', 'the', 'same', 'to', 'avoid', 'violating', 'the', 'matrix', 'sintegrity', 'interesting', 'to', 'note', 'that', 'for', 'higher', 'correlation', 'of', 'the', 'power', 'varincreases', 'which', 'is', 'counter', 'to', 'intuition', 'this', 'implies', 'that', 'we', 'need', 'to', 'lookinto', 'how', 'the', 'correlations', 'are', 'being', 'applied', 'within', 'the', 'model', 'once', 'we', 'canderive', 'single', 'correlations', 'from', 'the', 'term', 'structure', 'is', 'the', 'next', 'action', 'tounderstand', 'how', 'they', 'are', 'being', 'applied', 'and', 'whether', 'the', 'model', 'captures', 'the', 'lvolatility', 'in', 'the', 'spread', 'option', 'deals', 'from', 'onwards', 'the', 'var', 'calculation', 'failed', 'oliver']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "# higher threshold fewer phrases.\n",
    "bigram = Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = Phrases(bigram[data_words], threshold=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_image_image', 'getting', 'together', 'for', 'the', 'holidays', 'is', 'something', 'we', 'all', 'enjoy', 'whether', 'it', 'gathering', 'with', 'old', 'friends', 'from', 'out', 'of', 'town', 'or', 'hanging', 'out', 'with', 'the', 'usual', 'gang', 'every', 'gathering', 'this', 'time', 'of', 'year', 'seems', 'little', 'more', 'special', 'we', 'at', 'miller_brewing', 'wish', 'you', 'many', 'happy', 'celebrations', 'this', 'season', 'and', 'thank', 'you', 'for', 'enjoying', 'those', 'occasions', 'responsibly', 'happy_holidays', 'image', 'this', 'mail', 'is', 'not', 'sent', 'unsolicited', 'you', 'subscribed', 'to', 'receive', 'information', 'from', 'miller_brewing', 'at', 'miller', 'web_site', 'or', 'event', 'must', 'be', 'or', 'older', 'to', 'visit', 'our', 'web_site', 'miller_brewing', 'co', 'milwaukee', 'wi', 'miller_brewing', 'company', 'milwaukee', 'wi', 'privacy_statement', 'image_image_image_image', 'this', 'message', 'was', 'sent', 'by', 'miller_brewing', 'company', 'using', 'responsys', 'interact', 'tm', 'click_here', 'if', 'you', 'prefer', 'not', 'to', 'receive', 'future', 'mail', 'from', 'miller_brewing', 'company', 'click_here', 'to', 'view', 'our', 'permission', 'marketing', 'policy', 'image']\n"
     ]
    }
   ],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[200]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop_words, make bigrams and lemmatize\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_image', 'image', 'get', 'together', 'holiday', 'enjoy', 'gather', 'old', 'friend', 'town', 'hang', 'usual', 'gang', 'gathering', 'time', 'year', 'seem', 'little', 'special', 'miller_brewing', 'wish', 'many', 'happy', 'celebration', 'season', 'thank', 'enjoy', 'occasion', 'responsibly', 'happy_holiday', 'image', 'mail', 'send', 'unsolicited', 'subscribe', 'receive', 'information', 'miller_brewe', 'miller', 'web_site', 'event', 'old', 'visit', 'web_site', 'miller_brewe', 'co', 'milwaukee', 'wi', 'miller_brewe', 'company', 'milwaukee', 'wi', 'privacy', 'statement', 'message', 'send', 'miller_brewe', 'company', 'use', 'responsy', 'interact', 'click', 'prefer', 'receive', 'future', 'mail', 'miller_brewe', 'company', 'click', 'view', 'permission', 'marketing', 'policy', 'image']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary and corpus both are needed for (LDA) topic modeling\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "mallet_path = 'C:\\\\Users\\\\saman\\\\Downloads\\\\mallet-2.0.8\\\\bin\\\\mallet'\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling\n",
    "# corpus, dictionary and number of topics required for LDA\n",
    "# alpha and eta are hyperparameters that affect sparsity of the topics\n",
    "# chunksize is the number of documents to be used in each training chunk\n",
    "# update_every determines how often the model parameters should be updated\n",
    "# passes is the total number of training passes\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=20,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.042*\"september\" + 0.033*\"feature\" + 0.025*\"license\" + 0.012*\"strength\" + 0.010*\"limitation\" + 0.010*\"unauthorized\" + 0.006*\"capable\" + 0.002*\"ifwe\" + 0.001*\"lp\" + 0.001*\"dual\"'), (1, '0.179*\"mail\" + 0.061*\"message\" + 0.031*\"et\" + 0.028*\"email\" + 0.019*\"table\" + 0.018*\"org\" + 0.018*\"address\" + 0.017*\"font\" + 0.017*\"delete\" + 0.014*\"master\"'), (2, '0.059*\"game\" + 0.053*\"open\" + 0.049*\"attempt\" + 0.032*\"davis\" + 0.020*\"vote\" + 0.019*\"measure\" + 0.015*\"monitor\" + 0.015*\"senate\" + 0.013*\"permit\" + 0.013*\"rest\"'), (3, '0.031*\"internal\" + 0.022*\"deny\" + 0.019*\"sitara\" + 0.015*\"maintenance\" + 0.013*\"capability\" + 0.012*\"pleased\" + 0.012*\"accurate\" + 0.012*\"outage\" + 0.011*\"call_pager\" + 0.011*\"instance\"'), (4, '0.342*\"ect\" + 0.147*\"hou\" + 0.121*\"enron\" + 0.047*\"corp\" + 0.023*\"enronxgate\" + 0.013*\"lon\" + 0.013*\"communication\" + 0.011*\"forward\" + 0.010*\"ees_ee\" + 0.008*\"student\"'), (5, '0.118*\"error\" + 0.003*\"immediately_notify\" + 0.000*\"error_occurre\" + 0.000*\"initialize\" + 0.000*\"commanual_intervention\" + 0.000*\"borland_databaseengine\" + 0.000*\"pasting_less\" + 0.000*\"try_inserte\" + 0.000*\"may_contain\" + 0.000*\"disclosure\"'), (6, '0.027*\"way\" + 0.017*\"click\" + 0.017*\"great\" + 0.013*\"image\" + 0.012*\"offer\" + 0.012*\"online\" + 0.012*\"program\" + 0.011*\"free\" + 0.011*\"email\" + 0.011*\"center\"'), (7, '0.070*\"house\" + 0.036*\"lunch\" + 0.034*\"dinner\" + 0.007*\"anytime\" + 0.000*\"gov\" + 0.000*\"fri\" + 0.000*\"texas\" + 0.000*\"vnf\" + 0.000*\"pgn\" + 0.000*\"nahou_msmsw\"'), (8, '0.031*\"kim\" + 0.012*\"bullet\" + 0.012*\"mcconnell\" + 0.009*\"tk_lohman\" + 0.008*\"remind\" + 0.004*\"makes_sense\" + 0.003*\"amerex\" + 0.002*\"audrey_robertson\" + 0.000*\"mmbtu\" + 0.000*\"tw\"'), (9, '0.038*\"business\" + 0.035*\"service\" + 0.026*\"company\" + 0.023*\"new\" + 0.019*\"report\" + 0.019*\"system\" + 0.017*\"trading\" + 0.015*\"provide\" + 0.015*\"market\" + 0.015*\"information\"'), (10, '0.051*\"peter\" + 0.026*\"motion\" + 0.020*\"shipper\" + 0.017*\"certificate\" + 0.016*\"extension\" + 0.012*\"sufficient\" + 0.008*\"coordination\" + 0.007*\"originator\" + 0.002*\"bug\" + 0.002*\"override_letter\"'), (11, '0.102*\"deal\" + 0.077*\"gas\" + 0.069*\"price\" + 0.037*\"volume\" + 0.029*\"index\" + 0.028*\"specific\" + 0.026*\"mw\" + 0.024*\"sell\" + 0.018*\"significant\" + 0.017*\"pipeline\"'), (12, '0.041*\"say\" + 0.037*\"power\" + 0.037*\"energy\" + 0.030*\"state\" + 0.025*\"market\" + 0.022*\"company\" + 0.019*\"california\" + 0.016*\"price\" + 0.013*\"year\" + 0.011*\"utility\"'), (13, '0.052*\"role\" + 0.045*\"memo\" + 0.041*\"rto\" + 0.039*\"directly\" + 0.027*\"attached\" + 0.025*\"active\" + 0.021*\"var\" + 0.020*\"agenda\" + 0.014*\"proceeding\" + 0.013*\"formal\"'), (14, '0.068*\"st\" + 0.028*\"sara\" + 0.019*\"paragraph\" + 0.015*\"gerald\" + 0.008*\"dean\" + 0.006*\"admin\" + 0.000*\"ohio\" + 0.000*\"isda\" + 0.000*\"law\" + 0.000*\"defendant\"'), (15, '0.023*\"thank\" + 0.021*\"agreement\" + 0.020*\"review\" + 0.019*\"change\" + 0.018*\"question\" + 0.016*\"regard\" + 0.016*\"transaction\" + 0.015*\"attach\" + 0.014*\"contract\" + 0.014*\"send\"'), (16, '0.418*\"com\" + 0.405*\"enron\" + 0.004*\"smith\" + 0.004*\"william\" + 0.004*\"aol\" + 0.003*\"hotmail\" + 0.003*\"net\" + 0.003*\"john\" + 0.003*\"pete_davis\" + 0.002*\"david\"'), (17, '0.033*\"priority\" + 0.013*\"spreadsheet\" + 0.000*\"beforethe\" + 0.000*\"gisbs\" + 0.000*\"master_agreement\" + 0.000*\"bush\" + 0.000*\"law\" + 0.000*\"environment\" + 0.000*\"gopusa\" + 0.000*\"assign\"'), (18, '0.015*\"get\" + 0.013*\"time\" + 0.013*\"know\" + 0.013*\"need\" + 0.011*\"go\" + 0.010*\"also\" + 0.009*\"work\" + 0.009*\"call\" + 0.009*\"make\" + 0.009*\"let\"'), (19, '0.069*\"university\" + 0.052*\"iso\" + 0.021*\"_\" + 0.019*\"inform\" + 0.018*\"thus\" + 0.018*\"caiso\" + 0.015*\"pende\" + 0.012*\"baja\" + 0.011*\"contractor\" + 0.009*\"import\"')]\n"
     ]
    }
   ],
   "source": [
    "# The weights reflect how important a keyword is to that topic.\n",
    "print(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2398cd6c937708e42b80c0dce25ca405d0c6827c5ebcbcac9817d73cd0bc02f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 ('dsc650')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
